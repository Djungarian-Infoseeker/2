{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJmQfgqhkd5B4Asni5ICnN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Djungarian-Infoseeker/2/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "-7rFTHSOHfb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc79d7b-5e7a-4fdd-d001-45b2c6eafb84"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a2N8MTreTxTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e87eeda-7002-404e-d599-0bbc06456354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'JAMC_20_0057'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 53 (delta 21), reused 22 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (53/53), 17.53 KiB | 17.53 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "/content/JAMC_20_0057\n"
          ]
        }
      ],
      "source": [
        "# 克隆仓库\n",
        "!git clone https://github.com/yingkaisha/JAMC_20_0057.git\n",
        "%cd JAMC_20_0057"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8 keras==2.8\n",
        "!pip install keras-unet-collection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIveU7ljUAOS",
        "outputId": "2504bb73-0552-43dc-c83d-c4d8db10887b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.8\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting keras==2.8\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.12.1)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.17.0)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.68.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.32.3)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n",
            "Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.25.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Collecting keras-unet-collection\n",
            "  Downloading keras_unet_collection-0.1.13-py3-none-any.whl.metadata (8.0 kB)\n",
            "Downloading keras_unet_collection-0.1.13-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-unet-collection\n",
            "Successfully installed keras-unet-collection-0.1.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall netcdf4 xarray -y\n",
        "!pip install netcdf4==1.6.4 xarray==2023.1.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyb8_U3_wlkz",
        "outputId": "353dc494-b07f-4de1-abf1-7e52626d5c16"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping netcdf4 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: xarray 2024.11.0\n",
            "Uninstalling xarray-2024.11.0:\n",
            "  Successfully uninstalled xarray-2024.11.0\n",
            "Collecting netcdf4==1.6.4\n",
            "  Downloading netCDF4-1.6.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting xarray==2023.1.0\n",
            "  Downloading xarray-2023.1.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting cftime (from netcdf4==1.6.4)\n",
            "  Downloading cftime-1.6.4.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from netcdf4==1.6.4) (2024.12.14)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from netcdf4==1.6.4) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from xarray==2023.1.0) (2.2.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from xarray==2023.1.0) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->xarray==2023.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->xarray==2023.1.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->xarray==2023.1.0) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3->xarray==2023.1.0) (1.17.0)\n",
            "Downloading netCDF4-1.6.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xarray-2023.1.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.1/973.1 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.4.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netcdf4, xarray\n",
            "Successfully installed cftime-1.6.4.post1 netcdf4-1.6.4 xarray-2023.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "\n",
        "era_path = '/content/drive/MyDrive/ERA_TEMP_2024_interpolated.nc'\n",
        "etopo_path = '/content/drive/MyDrive/ETOPO_MSM_interpolated.nc'\n",
        "\n",
        "# 打开 NetCDF 文件\n",
        "era_ds = xr.open_dataset(era_path, engine='netcdf4')\n",
        "etopo_ds = xr.open_dataset(etopo_path, engine='netcdf4')\n",
        "\n",
        "# 打印文件摘要\n",
        "print(era_ds)\n",
        "print(etopo_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-EwjSF48Iz-",
        "outputId": "4d2c4edf-8034-49eb-fcc9-6de23f7f40b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<xarray.Dataset>\n",
            "Dimensions:     (valid_time: 8784, latitude: 501, longitude: 481)\n",
            "Coordinates:\n",
            "  * valid_time  (valid_time) datetime64[ns] 2024-01-01 ... 2024-12-31T23:00:00\n",
            "  * latitude    (latitude) float32 47.4 47.35 47.3 47.25 ... 22.5 22.45 22.4\n",
            "  * longitude   (longitude) float32 120.0 120.1 120.1 ... 149.9 149.9 150.0\n",
            "Data variables:\n",
            "    t2m         (valid_time, latitude, longitude) float64 ...\n",
            "<xarray.Dataset>\n",
            "Dimensions:  (lat: 501, lon: 481)\n",
            "Coordinates:\n",
            "  * lat      (lat) float64 22.4 22.45 22.5 22.55 22.6 ... 47.25 47.3 47.35 47.4\n",
            "  * lon      (lon) float64 120.0 120.1 120.1 120.2 ... 149.8 149.9 149.9 150.0\n",
            "Data variables:\n",
            "    z        (lat, lon) float64 ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall protobuf -y\n",
        "!pip install protobuf==3.20.3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "j2ctt4r3y3rh",
        "outputId": "bb3f27d3-b570-4d3e-b7d3-988bac8de909"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: protobuf 4.25.5\n",
            "Uninstalling protobuf-4.25.5:\n",
            "  Successfully uninstalled protobuf-4.25.5\n",
            "Collecting protobuf==3.20.3\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
            "pandas-gbq 0.25.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "f4334b9218a1479eab7067b17aeafc51"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "\n",
        "# 文件路径\n",
        "era5_path = '/content/drive/MyDrive/ERA_TEMP_2024_interpolated.nc'\n",
        "msm_path = '/content/drive/MyDrive/MSM_TEMP_2024_cut.nc'\n",
        "\n",
        "# 打开数据集\n",
        "era5_ds = xr.open_dataset(era5_path)\n",
        "msm_ds = xr.open_dataset(msm_path)\n",
        "\n",
        "# 打印变量信息\n",
        "print(\"ERA5 数据集变量：\", era5_ds.variables.keys())\n",
        "print(\"MSM 数据集变量：\", msm_ds.variables.keys())\n",
        "\n",
        "# 检查变量名称\n",
        "era5_var = 't2m' if 't2m' in era5_ds.variables else list(era5_ds.variables.keys())[0]\n",
        "msm_var = 't2m' if 't2m' in msm_ds.variables else list(msm_ds.variables.keys())[0]\n",
        "\n",
        "print(f\"ERA5 变量名称: {era5_var}\")\n",
        "print(f\"MSM 变量名称: {msm_var}\")\n",
        "\n",
        "# 检查单位\n",
        "era5_units = era5_ds[era5_var].attrs.get('units', '未指定')\n",
        "msm_units = msm_ds[msm_var].attrs.get('units', '未指定')\n",
        "\n",
        "print(f\"ERA5 单位: {era5_units}\")\n",
        "print(f\"MSM 单位: {msm_units}\")\n",
        "# 检查时间维度\n",
        "era5_time = era5_ds['valid_time']\n",
        "msm_time = msm_ds['valid_time']\n",
        "\n",
        "print(f\"ERA5 时间起点: {str(era5_time[0].values)}\")\n",
        "print(f\"ERA5 时间终点: {str(era5_time[-1].values)}\")\n",
        "print(f\"ERA5 时间步长: {era5_time.diff(dim='valid_time').mean().values}\")\n",
        "\n",
        "print(f\"MSM 时间起点: {str(msm_time[0].values)}\")\n",
        "print(f\"MSM 时间终点: {str(msm_time[-1].values)}\")\n",
        "print(f\"MSM 时间步长: {msm_time.diff(dim='valid_time').mean().values}\")\n",
        "\n",
        "# 检查时间长度\n",
        "print(f\"ERA5 时间步数: {len(era5_time)}\")\n",
        "print(f\"MSM 时间步数: {len(msm_time)}\")\n",
        "\n",
        "# 比较时间是否一致\n",
        "if (era5_time.values == msm_time.values).all():\n",
        "    print(\"✅ 时间维度完全一致\")\n",
        "else:\n",
        "    print(\"❌ 时间维度不一致，请检查时间步长或起止时间\")\n",
        "# 检查经纬度\n",
        "era5_lat = era5_ds['latitude']\n",
        "era5_lon = era5_ds['longitude']\n",
        "\n",
        "msm_lat = msm_ds['latitude']\n",
        "msm_lon = msm_ds['longitude']\n",
        "\n",
        "# 经纬度范围和步长检查\n",
        "print(f\"ERA5 纬度范围: {era5_lat.values.min()} ~ {era5_lat.values.max()}\")\n",
        "print(f\"MSM 纬度范围: {msm_lat.values.min()} ~ {msm_lat.values.max()}\")\n",
        "\n",
        "print(f\"ERA5 经度范围: {era5_lon.values.min()} ~ {era5_lon.values.max()}\")\n",
        "print(f\"MSM 经度范围: {msm_lon.values.min()} ~ {msm_lon.values.max()}\")\n",
        "\n",
        "# 检查网格是否匹配\n",
        "if (era5_lat.values == msm_lat.values).all() and (era5_lon.values == msm_lon.values).all():\n",
        "    print(\"✅ 经纬度网格完全匹配\")\n",
        "else:\n",
        "    print(\"❌ 经纬度网格不匹配，需要进行插值\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEc3QyM-PxGH",
        "outputId": "2b669e40-6148-4a70-e325-4592f8f0fe6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERA5 数据集变量： KeysView(Frozen({'t2m': <xarray.Variable (valid_time: 8784, latitude: 501, longitude: 481)>\n",
            "[2116777104 values with dtype=float64]\n",
            "Attributes:\n",
            "    long_name:      Interpolated Temperature at 2 meters\n",
            "    units:          K\n",
            "    standard_name:  air_temperature, 'valid_time': <xarray.IndexVariable 'valid_time' (valid_time: 8784)>\n",
            "array(['2024-01-01T00:00:00.000000000', '2024-01-01T01:00:00.000000000',\n",
            "       '2024-01-01T02:00:00.000000000', ..., '2024-12-31T21:00:00.000000000',\n",
            "       '2024-12-31T22:00:00.000000000', '2024-12-31T23:00:00.000000000'],\n",
            "      dtype='datetime64[ns]'), 'latitude': <xarray.IndexVariable 'latitude' (latitude: 501)>\n",
            "array([47.4 , 47.35, 47.3 , ..., 22.5 , 22.45, 22.4 ], dtype=float32), 'longitude': <xarray.IndexVariable 'longitude' (longitude: 481)>\n",
            "array([120.    , 120.0625, 120.125 , ..., 149.875 , 149.9375, 150.    ],\n",
            "      dtype=float32)}))\n",
            "MSM 数据集变量： KeysView(Frozen({'longitude': <xarray.IndexVariable 'longitude' (longitude: 481)>\n",
            "array([120.    , 120.0625, 120.125 , ..., 149.875 , 149.9375, 150.    ],\n",
            "      dtype=float32)\n",
            "Attributes:\n",
            "    units:    degrees_east, 'latitude': <xarray.IndexVariable 'latitude' (latitude: 501)>\n",
            "array([47.4 , 47.35, 47.3 , ..., 22.5 , 22.45, 22.4 ], dtype=float32)\n",
            "Attributes:\n",
            "    units:    degrees_north, 'valid_time': <xarray.IndexVariable 'valid_time' (valid_time: 8784)>\n",
            "array(['2024-01-01T00:00:00.000000000', '2024-01-01T01:00:00.000000000',\n",
            "       '2024-01-01T02:00:00.000000000', ..., '2024-12-31T21:00:00.000000000',\n",
            "       '2024-12-31T22:00:00.000000000', '2024-12-31T23:00:00.000000000'],\n",
            "      dtype='datetime64[ns]'), 't2m': <xarray.Variable (valid_time: 8784, latitude: 501, longitude: 481)>\n",
            "[2116777104 values with dtype=float32]\n",
            "Attributes:\n",
            "    units:          K\n",
            "    long_name:      Temperature at 2 meters\n",
            "    standard_name:  air_temperature}))\n",
            "ERA5 变量名称: t2m\n",
            "MSM 变量名称: t2m\n",
            "ERA5 单位: K\n",
            "MSM 单位: K\n",
            "ERA5 时间起点: 2024-01-01T00:00:00.000000000\n",
            "ERA5 时间终点: 2024-12-31T23:00:00.000000000\n",
            "ERA5 时间步长: 3600000000000 nanoseconds\n",
            "MSM 时间起点: 2024-01-01T00:00:00.000000000\n",
            "MSM 时间终点: 2024-12-31T23:00:00.000000000\n",
            "MSM 时间步长: 3600000000000 nanoseconds\n",
            "ERA5 时间步数: 8784\n",
            "MSM 时间步数: 8784\n",
            "✅ 时间维度完全一致\n",
            "ERA5 纬度范围: 22.399999618530273 ~ 47.400001525878906\n",
            "MSM 纬度范围: 22.399999618530273 ~ 47.400001525878906\n",
            "ERA5 经度范围: 120.0 ~ 150.0\n",
            "MSM 经度范围: 120.0 ~ 150.0\n",
            "✅ 经纬度网格完全匹配\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 运行 UNET-AE 模型\n",
        "%cd JAMC_20_0057\n",
        "!python UNET-AE_train.py t2m all 3 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQZsBeIeSZ5u",
        "outputId": "65829f5b-079d-4eff-8633-65269102a014"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'JAMC_20_0057'\n",
            "/content/JAMC_20_0057\n",
            "2025-01-08 12:17:22.242592: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-01-08 12:17:22.242631: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "T2 hidden layer setup\n",
            "2025-01-08 12:17:24.546895: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2025-01-08 12:17:24.546937: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2025-01-08 12:17:24.546987: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (f257660592e7): /proc/driver/nvidia/version does not exist\n",
            "2025-01-08 12:17:24.547207: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/JAMC_20_0057/UNET-AE_train.py\", line 79, in <module>\n",
            "    T_LOSS = np.empty((int(epochs*L_train), 2)); T_LOSS[...] = np.nan\n",
            "NameError: name 'L_train' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainfiles = glob('/content/drive/MyDrive/npy_data/batch_train_{}_{}_*.npy'.format(VAR, sea))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "r6V1nPYEx2mo",
        "outputId": "b49ec2b7-c5c1-41e5-adef-e56b9eb777af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'glob' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-53e2d599e5bd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/npy_data/batch_train_{}_{}_*.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msea\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
          ]
        }
      ]
    }
  ]
}